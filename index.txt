1:"$Sreact.fragment"
2:I[3719,["161","static/chunks/161-2061caadb2018b27.js","874","static/chunks/874-196dbf0660d69360.js","560","static/chunks/560-dc25b2d8b8d763b3.js","177","static/chunks/app/layout-9c6e6b49c30ef992.js"],"ThemeProvider"]
3:I[768,["161","static/chunks/161-2061caadb2018b27.js","874","static/chunks/874-196dbf0660d69360.js","560","static/chunks/560-dc25b2d8b8d763b3.js","177","static/chunks/app/layout-9c6e6b49c30ef992.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["161","static/chunks/161-2061caadb2018b27.js","874","static/chunks/874-196dbf0660d69360.js","560","static/chunks/560-dc25b2d8b8d763b3.js","177","static/chunks/app/layout-9c6e6b49c30ef992.js"],"default"]
7:I[7437,["161","static/chunks/161-2061caadb2018b27.js","12","static/chunks/12-4b59b9520356f054.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-ca0c5d399a7cd12c.js","974","static/chunks/app/page-e0f4a122248c6a06.js"],"default"]
8:I[9507,["161","static/chunks/161-2061caadb2018b27.js","12","static/chunks/12-4b59b9520356f054.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-ca0c5d399a7cd12c.js","974","static/chunks/app/page-e0f4a122248c6a06.js"],"default"]
9:I[5218,["161","static/chunks/161-2061caadb2018b27.js","12","static/chunks/12-4b59b9520356f054.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-ca0c5d399a7cd12c.js","974","static/chunks/app/page-e0f4a122248c6a06.js"],"default"]
12:I[1990,["161","static/chunks/161-2061caadb2018b27.js","12","static/chunks/12-4b59b9520356f054.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-ca0c5d399a7cd12c.js","974","static/chunks/app/page-e0f4a122248c6a06.js"],"default"]
13:I[9665,[],"MetadataBoundary"]
15:I[9665,[],"OutletBoundary"]
18:I[4911,[],"AsyncMetadataOutlet"]
1a:I[9665,[],"ViewportBoundary"]
1c:I[6614,[],""]
:HL["/_next/static/css/10be8fe6697fffe3.css","style"]
a:T682,Large language models (LLMs) have demonstrated significant improvements in contextual understanding. However, their ability to attend to truly critical information during long-context reasoning and generation still falls behind the pace. Specifically, our preliminary experiments reveal that certain distracting patterns can misdirect the model's attention during inference, and removing these patterns substantially improves reasoning accuracy and generation quality. We attribute this phenomenon to spurious correlations in the training data, which obstruct the model's capacity to infer authentic causal instruction-response relationships. This phenomenon may induce redundant reasoning processes, potentially resulting in significant inference overhead and, more critically, the generation of erroneous or suboptimal responses. To mitigate this, we introduce a two-stage framework called Learning to Focus (LeaF) leveraging intervention-based inference to disentangle confounding factors. In the first stage, LeaF employs gradient-based comparisons with an advanced teacher to automatically identify confounding tokens based on causal relationships in the training corpus. Then, in the second stage, it prunes these tokens during distillation to enact intervention, aligning the student's attention with the teacher's focus distribution on truly critical context tokens. Experimental results demonstrate that LeaF not only achieves an absolute improvement in various mathematical reasoning and code generation benchmarks but also effectively suppresses attention to confounding tokens during inference, yielding a more interpretable and reliable reasoning model.b:T7f1,@inproceedings{guo2025learning,
  title = {Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning},
  author = {Guo, Yiju and Yang, Wenkai and Sun, Zexu and Ding, Ning and Liu, Zhiyuan and Lin, Yankai},
  booktitle = {NeurIPS 2025 Conference},
  year = {2025},
  month = {sep},
  abstract = {Large language models (LLMs) have demonstrated significant improvements in contextual understanding. However, their ability to attend to truly critical information during long-context reasoning and generation still falls behind the pace. Specifically, our preliminary experiments reveal that certain distracting patterns can misdirect the model's attention during inference, and removing these patterns substantially improves reasoning accuracy and generation quality. We attribute this phenomenon to spurious correlations in the training data, which obstruct the model's capacity to infer authentic causal instruction-response relationships. This phenomenon may induce redundant reasoning processes, potentially resulting in significant inference overhead and, more critically, the generation of erroneous or suboptimal responses. To mitigate this, we introduce a two-stage framework called Learning to Focus (LeaF) leveraging intervention-based inference to disentangle confounding factors. In the first stage, LeaF employs gradient-based comparisons with an advanced teacher to automatically identify confounding tokens based on causal relationships in the training corpus. Then, in the second stage, it prunes these tokens during distillation to enact intervention, aligning the student's attention with the teacher's focus distribution on truly critical context tokens. Experimental results demonstrate that LeaF not only achieves an absolute improvement in various mathematical reasoning and code generation benchmarks but also effectively suppresses attention to confounding tokens during inference, yielding a more interpretable and reliable reasoning model.},
  doi = {https://arxiv.org/abs/2506.07851}
}c:T77e,Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.d:T8e7,@article{yang2025laser,
  title = {LaSeR: Reinforcement Learning with Last-Token Self-Rewarding},
  author = {Yang, Wenkai and Liu, Weijie and Xie, Ruobing and Guo, Yiju and Wu, Lulu and Yang, Saiyong and Lin, Yankai},
  journal = {arXiv preprint arXiv:2510.14943},
  year = {2025},
  month = {sep},
  abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.},
  doi = {https://arxiv.org/abs/2510.14943}
}e:T659,Reinforcement Learning from Human Feedback (RLHF) has emerged as a standard and effective approach for training large language models (LLMs) with human preferences. In this framework, a learned reward model approximates human preferences and guides policy optimization, making it crucial to develop an accurate reward model. However, without the ``true'' reward function, challenges arise when the reward model is an imperfect proxy for human preference. Since the policy optimization continuously shifts the human preference training dataset's distribution. The fixed reward model suffers from this problem of off-distribution, especially the on policy methods. While collecting new preference data can mitigate this issue, it is costly and challenging to optimize. Thus, reusing the policy interaction samples becomes a possible way to further refine the reward model. To tackle these challenges, we introduce a novel method textbfUncertainty-textbfGradient based textbfData textbfAugmentation (textbfUGDA for short) to enhance reward modeling by leveraging policy samples to maintain on-distribution performance. Specifically, UGDA selects interaction samples based on the uncertainty of the reward ensembles and the gradient based influence of policy optimization. After the reward relabeling of selected samples, we use supervised learning to refine the reward ensembles, then get the retrained policy. Extensive experiments demonstrate that by leveraging UGDA to select a few samples without the costly human preference data collection, we can improve the ability of the policy and surpass the state-of-the-art methods.f:T7fa,@inproceedings{sun2025uncertainty,
  title = {Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback},
  author = {Sun, Zexu and Guo, Yiju and Lin, Yankai and Chen, Xu and Qi, Qi and Tang, Xing and Wen, Ji-Rong},
  booktitle = {ICLR 2025 Conference},
  year = {2025},
  month = {jan},
  abstract = {Reinforcement Learning from Human Feedback (RLHF) has emerged as a standard and effective approach for training large language models (LLMs) with human preferences. In this framework, a learned reward model approximates human preferences and guides policy optimization, making it crucial to develop an accurate reward model. However, without the ``true'' reward function, challenges arise when the reward model is an imperfect proxy for human preference. Since the policy optimization continuously shifts the human preference training dataset's distribution. The fixed reward model suffers from this problem of off-distribution, especially the on policy methods. While collecting new preference data can mitigate this issue, it is costly and challenging to optimize. Thus, reusing the policy interaction samples becomes a possible way to further refine the reward model. To tackle these challenges, we introduce a novel method \textbf{U}ncertainty-\textbf{G}radient based \textbf{D}ata \textbf{A}ugmentation (\textbf{UGDA} for short) to enhance reward modeling by leveraging policy samples to maintain on-distribution performance. Specifically, UGDA selects interaction samples based on the uncertainty of the reward ensembles and the gradient based influence of policy optimization. After the reward relabeling of selected samples, we use supervised learning to refine the reward ensembles, then get the retrained policy. Extensive experiments demonstrate that by leveraging UGDA to select a few samples without the costly human preference data collection, we can improve the ability of the policy and surpass the state-of-the-art methods.},
  doi = {https://openreview.net/pdf?id=iamWnRpMuQ}
}10:T4d7,Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the "3H" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.11:T695,@inproceedings{guo2024controllable,
  title = {Controllable preference optimization: Toward controllable multi-objective alignment},
  author = {Guo, Yiju and Cui, Ganqu and Yuan, Lifan and Ding, Ning and Sun, Zexu and Sun, Bowen and Chen, Huimin and Xie, Ruobing and Zhou, Jie and Lin, Yankai and others},
  booktitle = {EMNLP 2024 main conference},
  year = {2024},
  month = {sep},
  abstract = {Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the "3H" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.},
  doi = {https://arxiv.org/pdf/2402.19085}
}0:{"P":null,"b":"O1XdRZ70J9u_9Rv2dLiwb","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/10be8fe6697fffe3.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Yiju Guo","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"December 11, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Yiju Guo","title":"PhD Student","institution":"Gaoling School of Artificial Intelligence, Renmin University of China","avatar":"/bio.jpg"},"social":{"email":"yiju.zeroyre@gmail.com","location":"Beijing, China","location_url":"https://maps.google.com","location_details":["17th Floor, Lide Building,","No.59, Zhongguancun Street, Haidian District, Beijing, China"],"google_scholar":"https://scholar.google.com/citations?user=rwN83MAAAAAJ&hl=zh-CN","orcid":"https://dblp.org/pid/371/4656.html","github":"https://github.com/YijuGuo","linkedin":"https://www.linkedin.com/in/yijuguo"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["LLM Alignment & Reasoning","Reinforcement Learning","Natural Language Processing"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"I am a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China (RUC), advised by [Prof. Yankai Lin](https://linyankai.github.io/).\n\nMy research focuses on **Natural Language Processing (NLP)** and **Reinforcement Learning (RL)**. Specifically, I am interested in:\n* **LLM Alignment** (e.g., RLHF, Multi-Objective Optimization)\n* **Reasoning & Controlled Generation** (e.g., Causal Inference, Attention Mechanisms)","title":"About"}],["$","$L9","featured_publications",{"publications":[{"id":"guo2025learning","title":"Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning","authors":[{"name":"Yiju Guo","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenkai Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zexu Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ning Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiyuan Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yankai Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"9","type":"conference","status":"published","tags":["LLM Reasoning","Causal Attention","Token Pruning","Knowledge Distillation","Interpretability"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"transformer-architectures","journal":"","conference":"NeurIPS 2025 Conference","doi":"https://arxiv.org/abs/2506.07851","code":"https://github.com/RUCBM/LeaF","abstract":"$a","description":"A framework to improve LLM reasoning by removing distracting tokens via causal attention distillation and gradient-guided pruning.","selected":true,"preview":"leaf.png","bibtex":"$b"},{"id":"yang2025laser","title":"LaSeR: Reinforcement Learning with Last-Token Self-Rewarding","authors":[{"name":"Wenkai Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Weijie Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruobing Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiju Guo","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Lulu Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Saiyong Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yankai Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"9","type":"journal","status":"published","tags":["Reinforcement Learning","Self-Verification","LLM Reasoning","Efficient Inference","RLVR"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"arXiv preprint arXiv:2510.14943","conference":"","doi":"https://arxiv.org/abs/2510.14943","code":"https://github.com/RUCBM/LaSeR","abstract":"$c","description":"An efficient RL method that unifies reasoning and verification by utilizing the last-token probability as a self-rewarding signal.","selected":true,"preview":"laser.png","bibtex":"$d"},{"id":"sun2025uncertainty","title":"Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback","authors":[{"name":"Zexu Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiju Guo","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Yankai Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xu Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qi Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xing Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ji-Rong Wen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"1","type":"conference","status":"published","tags":["RLHF","Reward Modeling","Data Augmentation","Uncertainty Estimation","Sample Efficiency"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"ICLR 2025 Conference","doi":"https://openreview.net/pdf?id=iamWnRpMuQ","abstract":"$e","description":"An uncertainty-aware data augmentation method to refine reward models in RLHF without expensive human annotation.","selected":true,"preview":"us.png","bibtex":"$f"},{"id":"guo2024controllable","title":"Controllable preference optimization: Toward controllable multi-objective alignment","authors":[{"name":"Yiju Guo","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Ganqu Cui","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lifan Yuan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ning Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zexu Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bowen Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Huimin Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ruobing Xie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jie Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yankai Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"others","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"9","type":"conference","status":"published","tags":["LLM Alignment","Multi-Objective Optimization","RLHF","Controllable Generation","Preference Optimization"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"EMNLP 2024 main conference","doi":"https://arxiv.org/pdf/2402.19085","code":"https://github.com/OpenBMB/CPO","abstract":"$10","description":"A multi-objective alignment method that explicitly controls preference scores to balance helpfulness, honesty, and harmlessness.","selected":true,"preview":"cpo.png","bibtex":"$11"}],"title":"Selected Publications","enableOnePageMode":false}],["$","$L12","news",{"items":[{"date":"2025-09","content":"Our work ã€ŠLearning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruningã€‹has been accepted by NeurIPS 2025 main conference ðŸŽ‰"},{"date":"2025-01","content":"Our work ã€ŠUncertainty and influence aware reward model refinement for reinforcement learning from human feedbackã€‹has been accepted by ICLR 2025 ðŸŽ‰"},{"date":"2024-09","content":"Our work ã€ŠControllable preference optimization: Toward controllable multi-objective alignmentã€‹has been accepted by EMNLP 2024 main conference ðŸŽ‰"},{"date":"2022-09","content":"Starting my PhD at the Gaoling School of Artificial Intelligence, Renmin University of China (RUC)"}],"title":"News"}]],false,false,false]}]]}]]}]}],["$","$L13",null,{"children":"$L14"}],null,["$","$L15",null,{"children":["$L16","$L17",["$","$L18",null,{"promise":"$@19"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","TqCmYkmnj8IM22yLYyXD-",{"children":[["$","$L1a",null,{"children":"$L1b"}],null]}],null]}],false]],"m":"$undefined","G":["$1c","$undefined"],"s":false,"S":true}
1d:"$Sreact.suspense"
1e:I[4911,[],"AsyncMetadata"]
14:["$","$1d",null,{"fallback":null,"children":["$","$L1e",null,{"promise":"$@1f"}]}]
17:null
1b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:null
1f:{"metadata":[["$","title","0",{"children":"Yiju Guo"}],["$","meta","1",{"name":"description","content":"PhD student at Renmin University of China."}],["$","meta","2",{"name":"author","content":"Yiju Guo"}],["$","meta","3",{"name":"keywords","content":"Yiju Guo,PhD,Research,Gaoling School of Artificial Intelligence, Renmin University of China"}],["$","meta","4",{"name":"creator","content":"Yiju Guo"}],["$","meta","5",{"name":"publisher","content":"Yiju Guo"}],["$","meta","6",{"property":"og:title","content":"Yiju Guo"}],["$","meta","7",{"property":"og:description","content":"PhD student at Renmin University of China."}],["$","meta","8",{"property":"og:site_name","content":"Yiju Guo's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Yiju Guo"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at Renmin University of China."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
19:{"metadata":"$1f:metadata","error":null,"digest":"$undefined"}
