<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/10be8fe6697fffe3.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-50096e198fb40285.js" async=""></script><script src="/_next/static/chunks/main-app-9eafb1322f0dab38.js" async=""></script><script src="/_next/static/chunks/161-2061caadb2018b27.js" async=""></script><script src="/_next/static/chunks/874-196dbf0660d69360.js" async=""></script><script src="/_next/static/chunks/560-dc25b2d8b8d763b3.js" async=""></script><script src="/_next/static/chunks/app/layout-9c6e6b49c30ef992.js" async=""></script><script src="/_next/static/chunks/12-4b59b9520356f054.js" async=""></script><script src="/_next/static/chunks/748-ca0c5d399a7cd12c.js" async=""></script><script src="/_next/static/chunks/app/page-e0f4a122248c6a06.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Yiju Guo</title><meta name="description" content="PhD student at Renmin University of China."/><meta name="author" content="Yiju Guo"/><meta name="keywords" content="Yiju Guo,PhD,Research,Gaoling School of Artificial Intelligence, Renmin University of China"/><meta name="creator" content="Yiju Guo"/><meta name="publisher" content="Yiju Guo"/><meta property="og:title" content="Yiju Guo"/><meta property="og:description" content="PhD student at Renmin University of China."/><meta property="og:site_name" content="Yiju Guo&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Yiju Guo"/><meta name="twitter:description" content="PhD student at Renmin University of China."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Yiju Guo</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Yiju Guo" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Yiju Guo</h1><p class="text-lg text-accent font-medium mb-1">PhD Student</p><p class="text-neutral-600 mb-2">Gaoling School of Artificial Intelligence, Renmin University of China</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=rwN83MAAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://dblp.org/pid/371/4656.html" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/YijuGuo" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.linkedin.com/in/yijuguo" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>LLM Alignment &amp; Reasoning</div><div>Reinforcement Learning</div><div>Natural Language Processing</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China (RUC), advised by <a href="https://linyankai.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Yankai Lin</a>. I am also conducting research at Natural Language Processing Lab at <a href="https://nlp.csai.tsinghua.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Tsinghua University(THUNLP)</a>, supervised by <a href="https://www.stingning.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Ning Ding</a>.</p>
<p class="mb-4 last:mb-0">My research focuses on <strong class="font-semibold text-primary">Large Language Model (LLM)</strong> and <strong class="font-semibold text-primary">Reinforcement Learning (RL)</strong>. Specifically, I am interested in:</p>
<ul class="list-disc list-inside mb-4 space-y-1 ml-4">
<li class="mb-1"><strong class="font-semibold text-primary">LLM Alignment</strong> (e.g., RLHF, Multi-Objective Optimization)</li>
<li class="mb-1"><strong class="font-semibold text-primary">Reasoning &amp; Generation</strong> (e.g., Causal Inference, Attention Mechanisms)</li>
</ul></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All â†’</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Yiju Guo</span>, </span><span><span class=" ">Wenkai Yang</span>, </span><span><span class=" ">Zexu Sun</span>, </span><span><span class=" ">Ning Ding</span>, </span><span><span class=" ">Zhiyuan Liu</span>, </span><span><span class=" ">Yankai Lin</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">NeurIPS 2025 Conference</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A framework to improve LLM reasoning by removing distracting tokens via causal attention distillation and gradient-guided pruning.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class=" ">Wenkai Yang</span>, </span><span><span class=" ">Weijie Liu</span>, </span><span><span class=" ">Ruobing Xie</span>, </span><span><span class="font-semibold text-accent ">Yiju Guo</span>, </span><span><span class=" ">Lulu Wu</span>, </span><span><span class=" ">Saiyong Yang</span>, </span><span><span class=" ">Yankai Lin</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">arXiv preprint arXiv:2510.14943</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">An efficient RL method that unifies reasoning and verification by utilizing the last-token probability as a self-rewarding signal.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class=" ">Zexu Sun</span>, </span><span><span class="font-semibold text-accent ">Yiju Guo</span>, </span><span><span class=" ">Yankai Lin</span>, </span><span><span class=" ">Xu Chen</span>, </span><span><span class=" ">Qi Qi</span>, </span><span><span class=" ">Xing Tang</span>, </span><span><span class=" ">Ji-Rong Wen</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ICLR 2025 Conference</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">An uncertainty-aware data augmentation method to refine reward models in RLHF without expensive human annotation.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Controllable preference optimization: Toward controllable multi-objective alignment</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Yiju Guo</span>, </span><span><span class=" ">Ganqu Cui</span>, </span><span><span class=" ">Lifan Yuan</span>, </span><span><span class=" ">Ning Ding</span>, </span><span><span class=" ">Zexu Sun</span>, </span><span><span class=" ">Bowen Sun</span>, </span><span><span class=" ">Huimin Chen</span>, </span><span><span class=" ">Ruobing Xie</span>, </span><span><span class=" ">Jie Zhou</span>, </span><span><span class=" ">Yankai Lin</span>, </span><span><span class=" ">others</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">EMNLP 2024 main conference</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A multi-objective alignment method that explicitly controls preference scores to balance helpfulness, honesty, and harmlessness.</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-09</span><p class="text-sm text-neutral-700">Our work ã€ŠLearning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruningã€‹has been accepted by NeurIPS 2025 main conference ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-01</span><p class="text-sm text-neutral-700">Our work ã€ŠUncertainty and influence aware reward model refinement for reinforcement learning from human feedbackã€‹has been accepted by ICLR 2025 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-09</span><p class="text-sm text-neutral-700">Our work ã€ŠControllable preference optimization: Toward controllable multi-objective alignmentã€‹has been accepted by EMNLP 2024 main conference ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2022-09</span><p class="text-sm text-neutral-700">Starting my PhD at the Gaoling School of Artificial Intelligence, Renmin University of China (RUC)</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->December 11, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"560\",\"static/chunks/560-dc25b2d8b8d763b3.js\",\"177\",\"static/chunks/app/layout-9c6e6b49c30ef992.js\"],\"ThemeProvider\"]\n3:I[768,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"560\",\"static/chunks/560-dc25b2d8b8d763b3.js\",\"177\",\"static/chunks/app/layout-9c6e6b49c30ef992.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"560\",\"static/chunks/560-dc25b2d8b8d763b3.js\",\"177\",\"static/chunks/app/layout-9c6e6b49c30ef992.js\"],\"default\"]\n7:I[7437,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"12\",\"static/chunks/12-4b59b9520356f054.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"748\",\"static/chunks/748-ca0c5d399a7cd12c.js\",\"974\",\"static/chunks/app/page-e0f4a122248c6a06.js\"],\"default\"]\n8:I[9507,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"12\",\"static/chunks/12-4b59b9520356f054.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"748\",\"static/chunks/748-ca0c5d399a7cd12c.js\",\"974\",\"static/chunks/app/page-e0f4a122248c6a06.js\"],\"default\"]\n9:I[5218,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"12\",\"static/chunks/12-4b59b9520356f054.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"748\",\"static/chunks/748-ca0c5d399a7cd12c.js\",\"974\",\"static/chunks/app/page-e0f4a122248c6a06.js\"],\"default\"]\n12:I[1990,[\"161\",\"static/chunks/161-2061caadb2018b27.js\",\"12\",\"static/chunks/12-4b59b9520356f054.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"748\",\"static/chunks/748-ca0c5d399a7cd12c.js\",\"974\",\"static/chunks/app/page-e0f4a122248c6a06.js\"],\"default\"]\n13:I[9665,[],\"MetadataBoundary\"]\n15:I[9665,[],\"OutletBoundary\"]\n18:I[4911,[],\"AsyncMetadataOutlet\"]\n1a:I[9665,[],\"ViewportBoundary\"]\n1c:I[6614,[],\"\"]\n:HL[\"/_next/static/css/10be8fe6697fffe3.css\",\"style\"]\na:T682,Large language models (LLMs) have demonstrated significant improvements in contextual understanding. However, their abi"])</script><script>self.__next_f.push([1,"lity to attend to truly critical information during long-context reasoning and generation still falls behind the pace. Specifically, our preliminary experiments reveal that certain distracting patterns can misdirect the model's attention during inference, and removing these patterns substantially improves reasoning accuracy and generation quality. We attribute this phenomenon to spurious correlations in the training data, which obstruct the model's capacity to infer authentic causal instruction-response relationships. This phenomenon may induce redundant reasoning processes, potentially resulting in significant inference overhead and, more critically, the generation of erroneous or suboptimal responses. To mitigate this, we introduce a two-stage framework called Learning to Focus (LeaF) leveraging intervention-based inference to disentangle confounding factors. In the first stage, LeaF employs gradient-based comparisons with an advanced teacher to automatically identify confounding tokens based on causal relationships in the training corpus. Then, in the second stage, it prunes these tokens during distillation to enact intervention, aligning the student's attention with the teacher's focus distribution on truly critical context tokens. Experimental results demonstrate that LeaF not only achieves an absolute improvement in various mathematical reasoning and code generation benchmarks but also effectively suppresses attention to confounding tokens during inference, yielding a more interpretable and reliable reasoning model.b:T7f1,@inproceedings{guo2025learning,\n  title = {Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning},\n  author = {Guo, Yiju and Yang, Wenkai and Sun, Zexu and Ding, Ning and Liu, Zhiyuan and Lin, Yankai},\n  booktitle = {NeurIPS 2025 Conference},\n  year = {2025},\n  month = {sep},\n  abstract = {Large language models (LLMs) have demonstrated significant improvements in contextual understanding. However, their ability to attend to truly critical information during lon"])</script><script>self.__next_f.push([1,"g-context reasoning and generation still falls behind the pace. Specifically, our preliminary experiments reveal that certain distracting patterns can misdirect the model's attention during inference, and removing these patterns substantially improves reasoning accuracy and generation quality. We attribute this phenomenon to spurious correlations in the training data, which obstruct the model's capacity to infer authentic causal instruction-response relationships. This phenomenon may induce redundant reasoning processes, potentially resulting in significant inference overhead and, more critically, the generation of erroneous or suboptimal responses. To mitigate this, we introduce a two-stage framework called Learning to Focus (LeaF) leveraging intervention-based inference to disentangle confounding factors. In the first stage, LeaF employs gradient-based comparisons with an advanced teacher to automatically identify confounding tokens based on causal relationships in the training corpus. Then, in the second stage, it prunes these tokens during distillation to enact intervention, aligning the student's attention with the teacher's focus distribution on truly critical context tokens. Experimental results demonstrate that LeaF not only achieves an absolute improvement in various mathematical reasoning and code generation benchmarks but also effectively suppresses attention to confounding tokens during inference, yielding a more interpretable and reliable reasoning model.},\n  doi = {https://arxiv.org/abs/2506.07851}\n}c:T77e,Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self"])</script><script>self.__next_f.push([1,"-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.d:T8e7,"])</script><script>self.__next_f.push([1,"@article{yang2025laser,\n  title = {LaSeR: Reinforcement Learning with Last-Token Self-Rewarding},\n  author = {Yang, Wenkai and Liu, Weijie and Xie, Ruobing and Guo, Yiju and Wu, Lulu and Yang, Saiyong and Lin, Yankai},\n  journal = {arXiv preprint arXiv:2510.14943},\n  year = {2025},\n  month = {sep},\n  abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.},\n  doi = {https://arxiv.org/abs/2510.14943}\n}"])</script><script>self.__next_f.push([1,"e:T659,Reinforcement Learning from Human Feedback (RLHF) has emerged as a standard and effective approach for training large language models (LLMs) with human preferences. In this framework, a learned reward model approximates human preferences and guides policy optimization, making it crucial to develop an accurate reward model. However, without the ``true'' reward function, challenges arise when the reward model is an imperfect proxy for human preference. Since the policy optimization continuously shifts the human preference training dataset's distribution. The fixed reward model suffers from this problem of off-distribution, especially the on policy methods. While collecting new preference data can mitigate this issue, it is costly and challenging to optimize. Thus, reusing the policy interaction samples becomes a possible way to further refine the reward model. To tackle these challenges, we introduce a novel method textbfUncertainty-textbfGradient based textbfData textbfAugmentation (textbfUGDA for short) to enhance reward modeling by leveraging policy samples to maintain on-distribution performance. Specifically, UGDA selects interaction samples based on the uncertainty of the reward ensembles and the gradient based influence of policy optimization. After the reward relabeling of selected samples, we use supervised learning to refine the reward ensembles, then get the retrained policy. Extensive experiments demonstrate that by leveraging UGDA to select a few samples without the costly human preference data collection, we can improve the ability of the policy and surpass the state-of-the-art methods.f:T7fa,@inproceedings{sun2025uncertainty,\n  title = {Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback},\n  author = {Sun, Zexu and Guo, Yiju and Lin, Yankai and Chen, Xu and Qi, Qi and Tang, Xing and Wen, Ji-Rong},\n  booktitle = {ICLR 2025 Conference},\n  year = {2025},\n  month = {jan},\n  abstract = {Reinforcement Learning from Human Feedback (RLHF) has emerged "])</script><script>self.__next_f.push([1,"as a standard and effective approach for training large language models (LLMs) with human preferences. In this framework, a learned reward model approximates human preferences and guides policy optimization, making it crucial to develop an accurate reward model. However, without the ``true'' reward function, challenges arise when the reward model is an imperfect proxy for human preference. Since the policy optimization continuously shifts the human preference training dataset's distribution. The fixed reward model suffers from this problem of off-distribution, especially the on policy methods. While collecting new preference data can mitigate this issue, it is costly and challenging to optimize. Thus, reusing the policy interaction samples becomes a possible way to further refine the reward model. To tackle these challenges, we introduce a novel method \\textbf{U}ncertainty-\\textbf{G}radient based \\textbf{D}ata \\textbf{A}ugmentation (\\textbf{UGDA} for short) to enhance reward modeling by leveraging policy samples to maintain on-distribution performance. Specifically, UGDA selects interaction samples based on the uncertainty of the reward ensembles and the gradient based influence of policy optimization. After the reward relabeling of selected samples, we use supervised learning to refine the reward ensembles, then get the retrained policy. Extensive experiments demonstrate that by leveraging UGDA to select a few samples without the costly human preference data collection, we can improve the ability of the policy and surpass the state-of-the-art methods.},\n  doi = {https://openreview.net/pdf?id=iamWnRpMuQ}\n}10:T4d7,Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing a"])</script><script>self.__next_f.push([1,"lignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.11:T695,@inproceedings{guo2024controllable,\n  title = {Controllable preference optimization: Toward controllable multi-objective alignment},\n  author = {Guo, Yiju and Cui, Ganqu and Yuan, Lifan and Ding, Ning and Sun, Zexu and Sun, Bowen and Chen, Huimin and Xie, Ruobing and Zhou, Jie and Lin, Yankai and others},\n  booktitle = {EMNLP 2024 main conference},\n  year = {2024},\n  month = {sep},\n  abstract = {Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that"])</script><script>self.__next_f.push([1," meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.},\n  doi = {https://arxiv.org/pdf/2402.19085}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"IATVyD8v7Pexuz3nImRYE\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/10be8fe6697fffe3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Yiju Guo\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"December 11, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Yiju Guo\",\"title\":\"PhD Student\",\"institution\":\"Gaoling School of Artificial Intelligence, Renmin University of China\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"yiju.zeroyre@gmail.com\",\"location\":\"Beijing, China\",\"location_url\":\"https://maps.google.com\",\"location_details\":[\"17th Floor, Lide Building,\",\"No.59, Zhongguancun Street, Haidian District, Beijing, China\"],\"google_scholar\":\"https://scholar.google.com/citations?user=rwN83MAAAAAJ\u0026hl=zh-CN\",\"orcid\":\"https://dblp.org/pid/371/4656.html\",\"github\":\"https://github.com/YijuGuo\",\"linkedin\":\"https://www.linkedin.com/in/yijuguo\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"LLM Alignment \u0026 Reasoning\",\"Reinforcement Learning\",\"Natural Language Processing\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China (RUC), advised by [Prof. Yankai Lin](https://linyankai.github.io/). I am also conducting research at Natural Language Processing Lab at [Tsinghua University(THUNLP)](https://nlp.csai.tsinghua.edu.cn/), supervised by [Prof. Ning Ding](https://www.stingning.cn/).\\n\\nMy research focuses on **Large Language Model (LLM)** and **Reinforcement Learning (RL)**. Specifically, I am interested in:\\n* **LLM Alignment** (e.g., RLHF, Multi-Objective Optimization)\\n* **Reasoning \u0026 Generation** (e.g., Causal Inference, Attention Mechanisms)\",\"title\":\"About\"}],[\"$\",\"$L9\",\"featured_publications\",{\"publications\":[{\"id\":\"guo2025learning\",\"title\":\"Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning\",\"authors\":[{\"name\":\"Yiju Guo\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenkai Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zexu Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ning Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiyuan Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yankai Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"LLM Reasoning\",\"Causal Attention\",\"Token Pruning\",\"Knowledge Distillation\",\"Interpretability\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"NeurIPS 2025 Conference\",\"doi\":\"https://arxiv.org/abs/2506.07851\",\"code\":\"https://github.com/RUCBM/LeaF\",\"abstract\":\"$a\",\"description\":\"A framework to improve LLM reasoning by removing distracting tokens via causal attention distillation and gradient-guided pruning.\",\"selected\":true,\"preview\":\"leaf.png\",\"bibtex\":\"$b\"},{\"id\":\"yang2025laser\",\"title\":\"LaSeR: Reinforcement Learning with Last-Token Self-Rewarding\",\"authors\":[{\"name\":\"Wenkai Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Weijie Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruobing Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiju Guo\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lulu Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Saiyong Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yankai Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Reinforcement Learning\",\"Self-Verification\",\"LLM Reasoning\",\"Efficient Inference\",\"RLVR\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2510.14943\",\"conference\":\"\",\"doi\":\"https://arxiv.org/abs/2510.14943\",\"code\":\"https://github.com/RUCBM/LaSeR\",\"abstract\":\"$c\",\"description\":\"An efficient RL method that unifies reasoning and verification by utilizing the last-token probability as a self-rewarding signal.\",\"selected\":true,\"preview\":\"laser.png\",\"bibtex\":\"$d\"},{\"id\":\"sun2025uncertainty\",\"title\":\"Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback\",\"authors\":[{\"name\":\"Zexu Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiju Guo\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yankai Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xu Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qi Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xing Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ji-Rong Wen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"1\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"RLHF\",\"Reward Modeling\",\"Data Augmentation\",\"Uncertainty Estimation\",\"Sample Efficiency\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ICLR 2025 Conference\",\"doi\":\"https://openreview.net/pdf?id=iamWnRpMuQ\",\"abstract\":\"$e\",\"description\":\"An uncertainty-aware data augmentation method to refine reward models in RLHF without expensive human annotation.\",\"selected\":true,\"preview\":\"us.png\",\"bibtex\":\"$f\"},{\"id\":\"guo2024controllable\",\"title\":\"Controllable preference optimization: Toward controllable multi-objective alignment\",\"authors\":[{\"name\":\"Yiju Guo\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ganqu Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lifan Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ning Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zexu Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bowen Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Huimin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruobing Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jie Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yankai Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"others\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"9\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"LLM Alignment\",\"Multi-Objective Optimization\",\"RLHF\",\"Controllable Generation\",\"Preference Optimization\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"EMNLP 2024 main conference\",\"doi\":\"https://arxiv.org/pdf/2402.19085\",\"code\":\"https://github.com/OpenBMB/CPO\",\"abstract\":\"$10\",\"description\":\"A multi-objective alignment method that explicitly controls preference scores to balance helpfulness, honesty, and harmlessness.\",\"selected\":true,\"preview\":\"cpo.png\",\"bibtex\":\"$11\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}],[\"$\",\"$L12\",\"news\",{\"items\":[{\"date\":\"2025-09\",\"content\":\"Our work ã€ŠLearning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruningã€‹has been accepted by NeurIPS 2025 main conference ðŸŽ‰\"},{\"date\":\"2025-01\",\"content\":\"Our work ã€ŠUncertainty and influence aware reward model refinement for reinforcement learning from human feedbackã€‹has been accepted by ICLR 2025 ðŸŽ‰\"},{\"date\":\"2024-09\",\"content\":\"Our work ã€ŠControllable preference optimization: Toward controllable multi-objective alignmentã€‹has been accepted by EMNLP 2024 main conference ðŸŽ‰\"},{\"date\":\"2022-09\",\"content\":\"Starting my PhD at the Gaoling School of Artificial Intelligence, Renmin University of China (RUC)\"}],\"title\":\"News\"}]],false,false,false]}]]}]]}]}],[\"$\",\"$L13\",null,{\"children\":\"$L14\"}],null,[\"$\",\"$L15\",null,{\"children\":[\"$L16\",\"$L17\",[\"$\",\"$L18\",null,{\"promise\":\"$@19\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"EgQVzRfnNI1aywhwV5wC5\",{\"children\":[[\"$\",\"$L1a\",null,{\"children\":\"$L1b\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$1c\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1d:\"$Sreact.suspense\"\n1e:I[4911,[],\"AsyncMetadata\"]\n14:[\"$\",\"$1d\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1e\",null,{\"promise\":\"$@1f\"}]}]\n"])</script><script>self.__next_f.push([1,"17:null\n"])</script><script>self.__next_f.push([1,"1b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n16:null\n"])</script><script>self.__next_f.push([1,"1f:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Yiju Guo\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at Renmin University of China.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Yiju Guo\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Yiju Guo,PhD,Research,Gaoling School of Artificial Intelligence, Renmin University of China\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Yiju Guo\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Yiju Guo\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Yiju Guo\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at Renmin University of China.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Yiju Guo's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Yiju Guo\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at Renmin University of China.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\n19:{\"metadata\":\"$1f:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>